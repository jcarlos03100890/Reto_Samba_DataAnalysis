{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Created on Sun Jan 28 21:02:51 2024\n",
        "\n",
        "@author: juan_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tema 1: Conocimientos sobre Pandas\n",
        "\n",
        "## 1. Objetivo\n",
        "\n",
        "Procesar en Python la informaci\u00f3n entregada por el equipo de Ingenier\u00eda de datos de Oilst de forma funcional para el an\u00e1lisis de los retrasos en las \u00f3rdenes de los clientes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este documento se desarrollar\u00e1n scripts en Python que permitan procesar la  la informaci\u00f3n de Oilst\n",
        "para realizar posteriormente el an\u00e1lisis de sus datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Librerias de trabajo.\n",
        "\n",
        "Utilizaremos las siguientes librerias de python para procesar los archivos y crear un archivo con datos consolidados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from funciones import set_data_path,check_results_folder\n",
        "warnings.filterwarnings('ignore')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Lectura de datos\n",
        "\n",
        "Primero nos encargaremos de leer los datos, indicando a Python donde se encuentra\n",
        "la carpeta que contiene los datos y los nombres de los archivos relevantes para el an\u00e1lisis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#  Indicamos la ruta a la carpeta de los archivos a procesarse de los datos del E-commerce\n",
        "\n",
        "# Ya que los archivos se encuentran en el mismo folder solo en la carpeta inputs,\n",
        "# utilizamos la funcion set_data_path para identificar la carpeta donde estan los archivos a trabajar\n",
        "\n",
        "# Set the file path\n",
        "DATA_PATH = set_data_path(\"inputs\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora procederemos a definir variables que indiquen el nombre de los archivos junto con\n",
        "su extensi\u00f3n (por ejemplo, `.csv`, `.json` u otra)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "FILE_CUSTOMERS = 'olist_customers_dataset.xlsx'\n",
        "FILE_GEOLOCATIONS = 'olist_geolocation_dataset.csv'\n",
        "\n",
        "# completa los nombres del resto de los archivos con su extesion (ejemplo .csv) ...\n",
        "FILE_ITEMS = 'olist_order_items_dataset.csv'\n",
        "FILE_PAYMENTS = 'olist_order_payments_dataset.csv'\n",
        "FILE_ORDERS = 'olist_orders_dataset.csv'\n",
        "FILE_STATES_ABBREVIATIONS = 'states_abbreviations.json'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "USaremos de la utilidad `os.path.join` de Python que indicar la ruta de\n",
        " donde se ubican archivos, as\u00ed Pandas encontr\u00e1 los archivos de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leemos con pandas FILE_GEOLOCATIONS\n",
        "geolocations = pd.read_csv(\n",
        "    os.path.join(DATA_PATH, FILE_GEOLOCATIONS),\n",
        "    dtype={'geolocation_zip_code_prefix': 'str'}\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Archivo olist_customers_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Leemos los datos de los clientes\n",
        "customers = pd.read_excel(\n",
        "    os.path.join(DATA_PATH, FILE_CUSTOMERS),\n",
        "    # Especificar el tipo de dato de customer_zip_code_prefix\n",
        "    dtype={'customer_zip_code_prefix': 'str'}\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Archivo olist_order_items_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "items = pd.read_csv(\n",
        "    # Completa la ubicacion usando os.path.join, DATA_PATH y\n",
        "    # el nombre del archivo FILE_ITEMS\n",
        "    os.path.join(DATA_PATH, FILE_ITEMS))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como sabemos, este conjunto contiene datos de los productos que contiene cada orden. Por ello, para el an\u00e1lisis nos interesar\u00e1 saber cual es la cantidad de productos en cada orden y el precio total de las mismas.\n",
        "\n",
        "Esto se puede calcular mediante agregaciones de Pandas (https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.core.groupby.DataFrameGroupBy.agg.html), que basicamente nos permite hacer c\u00e1lculos para un grupo en especial. En el ejemplo de inferior se muestra para cada `order_id` se cuenta la cantidad de productos (items) y el precio agregado de todos los art\u00edculos en las \u00f3rdenes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "items_agg = items.groupby(\n",
        "    ['order_id']).agg(\n",
        "        # conteo de producto\n",
        "        {'order_item_id': 'count',\n",
        "         # suma de los precios de los art\u00edculos\n",
        "         'price': 'sum'}\n",
        ").reset_index()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "items_agg.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a renombrar las columnas anteriores, para que sea m\u00e1s intuitivo su significado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Nota: el par\u00e1metro inplace sobre escribe los cambios\n",
        "# en el dataframe\n",
        "items_agg.rename(\n",
        "    columns={'order_item_id': 'total_products', 'price': 'total_sales'},\n",
        "    inplace=True\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "items_agg"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a coninuar leyendo los archivos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 olist_order_payments_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "payments = pd.read_csv(\n",
        "    # Completa la ubicacion usando os.path.join, DATA_PATH y\n",
        "    # el nombre del archivo correspondiente a los pagos\n",
        "    os.path.join(DATA_PATH, FILE_PAYMENTS)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 states_abbreviations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "states_abbreviations = pd.read_json(\n",
        "    # Completa la ubicacion usando os.path.join, DATA_PATH y\n",
        "    # el nombre del archivo correspondiente a las abreviaciones de los estados\n",
        "    os.path.join(DATA_PATH, FILE_STATES_ABBREVIATIONS)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 olist_orders_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "orders = pd.read_csv(\n",
        "    # Completa la ubicacion usando os.path.join, DATA_PATH y\n",
        "    # el nombre del archivo correspondiente a las ordenes de Oislt\n",
        "    os.path.join(DATA_PATH, FILE_ORDERS)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convierte a formato fecha completando los campos apropiados\n",
        "\n",
        "# convierte order_purchase_timestamp\n",
        "orders['order_purchase_timestamp'] = pd.to_datetime(\n",
        "    orders['order_purchase_timestamp'], errors='coerce')\n",
        "# order_approved_at\n",
        "orders['order_approved_at'] = pd.to_datetime(\n",
        "    orders['order_approved_at'], errors='coerce')\n",
        "\n",
        "# order_delivered_carrier_date\n",
        "orders['order_delivered_carrier_date'] = pd.to_datetime(\n",
        "    orders['order_delivered_carrier_date'], errors='coerce')\n",
        "\n",
        "# order_delivered_customer_date\n",
        "orders['order_delivered_customer_date'] = pd.to_datetime(\n",
        "    orders['order_delivered_customer_date'], errors='coerce')\n",
        "\n",
        "# order_estimated_delivery_date\n",
        "orders['order_estimated_delivery_date'] = pd.to_datetime(\n",
        "    orders['order_estimated_delivery_date'], errors='coerce')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agregamos funciones auxilares derivadas de las fechas de las ordenes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define una columna con el a\u00f1o en que sucedi\u00f3 la orden\n",
        "orders['year'] = orders['order_purchase_timestamp'].dt.year\n",
        "\n",
        "# Define una columna con el mes en que sucedi\u00f3 la orden\n",
        "orders['month'] = orders['order_purchase_timestamp'].dt.month\n",
        "\n",
        "# Define una columna con trimestre con el que paso la orden (ej. Q12018)\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.to_period.html\n",
        "orders['quarter'] = orders['order_purchase_timestamp'].dt.to_period('Q')\n",
        "\n",
        "# Define una columna con mes y a\u00f1o con el que paso la orden (ej. 02-2018)\n",
        "# Hint: \u00bfque hace el metodo ...to_period('M')?\n",
        "orders['year_month'] = orders['month'].astype(\n",
        "    str).str.zfill(2) + \"-\" + orders['year'].astype(str)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por otro lado, tambi\u00e9n necesitamos identificar las \u00f3rdenes que tuvieron retrasos prolongados.\n",
        "Recordemos que de acuerdo a la documentaci\u00f3n del `Anexo A`:\n",
        "\n",
        "* Oilst notifica el usuario de cuando llegar\u00e1 su pedido con el valor de la columna `order_estimated_delivery_date`,\n",
        "* Adem\u00e1s la fecha real en que se llev\u00f3 la entrega se encuentra en el campo `order_delivered_customer_date`\n",
        "\n",
        "A continuaci\u00f3n calcularemos distancia (en d\u00edas) entre ambas fecha definiendo a la variable `delta_days`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Nota: tenemos que realizar la conversion de\n",
        "# segundos a d\u00edas\n",
        "\n",
        "orders['delta_days'] = (\n",
        "    orders['order_delivered_customer_date'] -\n",
        "    orders['order_estimated_delivery_date']\n",
        ").dt.total_seconds() / 60 / 60 / 24"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el contexto del problema, los valores de `delta_days` tiene el significado:\n",
        "\n",
        "* Un valor negativo en `delta_days` significa que el pedido llego antes de lo esperado; es decir, no existi\u00f3 retraso.\n",
        "* Un valor de `delta_days`, mayor a 0 d\u00edas pero menor a 3 d\u00edas, significa que es un retrazo aceptable, \n",
        "* Sin embargo, si `delta_days` es m\u00e1s grande que 3 d\u00edas esto significa que tenemos un retrazo prolongado.\n",
        "\n",
        "Crearemos una variable `delay_status` para indicar la discusi\u00f3n anterior usando el operador `where` de Numpy (https://towardsdatascience.com/creating-conditional-columns-on-pandas-with-numpy-select-and-where-methods-8ee6e2dbd5d5).\n",
        "\n",
        "Esencialmente, el operador `where` de Numpy permite definir variables siguiendo reglas l\u00f3gicas de manera condicional, similar al `if ... else ...` de Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Definimos 'delay_status'\n",
        "orders['delay_status'] = np.where(\n",
        "    orders['delta_days'] > 3, 'long_delay',\n",
        "    np.where(orders['delta_days'] <= 0, 'on_time', 'short_delay')\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 olist_geolocation_dataset\n",
        "\n",
        "Aunque anteriormente hemos le\u00eddo este archivo, debemos notar que contiene informaci\u00f3n redudante de muchos codigos postales, como en el caso del valor `24220`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "geolocations.query(\"geolocation_zip_code_prefix == 24220\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para el an\u00e1lisis tendremos que eliminar esta duplicaciones. \n",
        "Esto se puede lograr con el m\u00e9todo `drop_duplicates`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "unique_geolocations = geolocations.drop_duplicates(\n",
        "    subset=['geolocation_zip_code_prefix']\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como se aprecia a continuaci\u00f3n, ahora el dataframe `unique_geolocations` corrige el error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "unique_geolocations.query(\n",
        "    \"geolocation_zip_code_prefix == 24220\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Procesamiento global\n",
        "\n",
        "Ahora que hemos cargado a Pandas los datos del E-commerce, debemos **consolidar toda la informaci\u00f3n** en una sola tabla, lo que nos permitir\u00e1 centralizar el an\u00e1lisis y hacer comparativos.\n",
        "\n",
        "Para ello, nos proponemos lo siguiente:\n",
        "    \n",
        "1. A los datos de clientes le a\u00f1adiremos los datos de geolocalizaci\u00f3n. **(Clientes + geolocalizaci\u00f3n)**\n",
        "2. Tales datos se complementar\u00e1n a\u00f1adiendo los datos del nombre del estado de Brasil en que se localizan.\n",
        "(**Clientes + geolocalizaci\u00f3n + nombre del estado donde viven**)\n",
        "3. Posteriormente archivo de \u00f3rdenes, agregaremos los datos del precio y cantidad de art\u00edculos.\n",
        "**(\u00d3rdenes + total de art\u00edculos y precios)**\n",
        "4. Finalmente, uniremos toda la informaci\u00f3n de los pasos 2 y 3 en una sola tabla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Clientes + geolocalizaci\u00f3n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customers_geolocation = customers.merge(\n",
        "    unique_geolocations,\n",
        "    left_on='customer_zip_code_prefix',\n",
        "    right_on='geolocation_zip_code_prefix',\n",
        "    how='left'\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customers_geolocation.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Clientes + geolocalizaci\u00f3n + nombre del estado donde viven\n",
        "\n",
        "Ahora repetiremos un proceso an\u00e1logo pero con los nombres del estado donde viven los customers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Une los dataframe customers_geolocation y states_abbreviations\n",
        "\n",
        "customers_geolocation_estado = customers_geolocation.merge(\n",
        "    states_abbreviations,\n",
        "    left_on='geolocation_state',\n",
        "    right_on='abbreviation',\n",
        "    how='left'\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customers_geolocation_estado"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 \u00d3rdenes + total de art\u00edculos y precios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# une los dataframe orders y items_agg por order_id\n",
        "orders_totals = orders.merge(\n",
        "    items_agg,\n",
        "    on=['order_id'],\n",
        "    how='left'\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "orders_totals"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Clientes + geolocalizaci\u00f3n + nombre del estado donde viven + \u00d3rdenes + total de art\u00edculos y precios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = orders_totals.merge(\n",
        "    customers_geolocation_estado,\n",
        "    on=['customer_id'],\n",
        "    how='left'\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results.info()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de guardar los resultados, verificacmos si existen las carpetas para guardar los resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "check_results_folder()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente escribiremos el resultado en un archivo separado por comas `.csv`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Completa el codigo\n",
        "results.to_csv(\n",
        "    # nombre del archivo\n",
        "    'results/1/oilst_processed.csv',\n",
        "    # flag para no escribir el indice del dataframe al csv\n",
        "    index=False\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}